
## Overview

- Who am i (And memories from Aber)
- what I do, and what I have done
- AI bad

- Skills you need regardless

## Script

Hi evreyone, and thanks for having me! I'm John and I actually graduate from
Aber uni in 2023, and have since worked as a developer, architect, subject manager
and tech lead. 

-- Continue with introduction?

## Advice #1 - Don't use AI*

Let's start with big swings, eh!

You should not be using AI to do your work. No code you write during your education should
ever be produced by an AI!

Now, if you have your tinfoil hats on you might think that this is ploy from the professors,
but no, this is a purely selfish advice. I don't care about your grades. Of course I hope
you do well and that you achieve the things you want to achieve, but that is of little
concern to me. No, I care about getting usable developers for interviews. The more of you
that becomes dependent on AI, the fewer usable graduates I get for technical interviews.

I know that sounds harsh, but it is the truth. I speak to collegues all over the world, and
all that is involved with hiring is saying the same: Graduates after LLMs hit the scene is
of a much lower quality than the ones before.

I want to be very clear though: I don't think you're worse, but I think your generation
of developrs has been given the tool to shoot yourself in the foot without even realizing it.
It is not your fault, and if I had LLMs while studying I know that I would fall into the same
trap. So again, to be very clear, this isn't a "this generation is dumber" argument, but rather
this generation has been sabotaged.

If you read on social media, you'll be fed with the idea that AI will kill software development
and soon all code will be written by AIs. We see these quotes all the time:

*Show and talk about quotes*

The problem with these quotes is that they're all coming from people that has vested interest in
making AI succesful. Let's look at the actual truth:

*Show and talk about studies*

Alright, so now that we pretty sure that AI isn't our lord and saviour, that begs the question:
what has actually changed when it comes to the requirements for a developer?

I would argue that very little has changed. We still need people that can learn the technologies
we use, not only on the surface level, but also on a deeper technical level. We still need people
who has a knack for architecture so that we have maintainable, robust, scalable and performant systems.

This has always been the case, and it is still the case. So how does one get these skills then? How does
one learn how to develop such systems?

Here I want to go back to the Measuring the Impact of Early-2025 AI on
Experienced Open-Source Developer Productivity study, and look at this graph

Easy: You build systems, you learn from mistakes, and you build up an intuition about what is right when
you write your code. The problem with AI is that it hijacks this learning process. Rather than building
your intution, it just generates something for you. It sits between you and the code in such a way
that it gets in the way of you learning. At best, AI pushes you towards stagnation.

Another thing to bear in mind is that learning is a skill that needs to be trained. Learning is really
hard when you're not used to learning. The problem here is that I need candidates that can learn, and even
do so effectively! That is one of the most important traits I'm looking for in a candidate!

Let me tell you a little secret about junior developers and interns: They are always a net negative on
productivity. They always hold the overall team back in one way or another. And let me clear: AI has nothing
to do with this. It is not even a thing I hold against juniors - I'm all for hiring them! But I don't hire
juniors because I really like having juniors. 

I hire juniors because they will eventually not be juniors anymore. They will grow and become mid-level
developers who can be productive without someone like me holding their hands. The one goal of a junior
is to learn and grow until they become a productive member of the team, and this is a process.
Senior developers know that everyone is different and learns at their own pace, and that is perfectly find and normal.

What we observe though, is that juniors that rely heavily on AI do not grow. They need a much longer time getting
to a stage where we trust them without supervising them. The worst juniors doesn't take accountability for the
code that the AI generated, and they put the burden on us seniors to review their code and make sure it won't
blow up the application.

So now, we're in a situation where the junior generates more code, with less understanding that puts a bigger
burden on the non-junior developers around them. Due to their AI usage, they will continue to do so for longer,
and for some they will never break the habit. Which means that they will never grow to a stage where they are as
productive as they could have been.

Your ability to learn and grow is one of the most important factors that decides whether I will give you a job
offer, and AI is counterproductive to that.

If you look at the title of this advice, you see that there's a caveat though. This isn't a dogamtic statement
without exceptons. You can use AI, but never to generate code that you'll actually end up using. Use the AI
as you would use a professor. Ask it questions, but remember that a professor will not just give you the
solution. You don't get to generate code with it.

If you're using an editor with a built-in AI auto-complete, disable it. If you have a subscription, cancel it. Don't
use AI agents or RAG or anything like that. Use AI like you would use a search engine. Ask questions, but
you write the code. Pretend that copy/paste doesn't exist. Prefer to put as much distance between your code and the AI
as possible.

Doing this forces you to learn. It forces you to write the code and as a result, you will grow as a developer. That learning
muscle will be exercised and you will overall be a stronger candidate for it.

Now, another advice here: When applying for jobs you might sometimes get take-home assignments. Don't use AI to
solve them. It is very obvious and it is super off-putting. Believe me, you won't trick anyone, and you will
look like an idiot during the interview. Let your skills speak for themselves.

After all, if you can't do work without an AI, why do I need you? Why not just outsource it to an AI agent instead? Why
do I need a junior as a middle-man?

I could go into more details about AI, but the short of it is that you should avoid using it as a shortcut to solve problems.

## Advice #2 - "Working" is the bare minimum

One counter argument to the whole AI thing is that they produce code that, technically, works.

Yeah, sure, sometimes, maybe, not always in the way we want them to work, but sure
they can spit out working low-complexity solutions here and there.

The problem is that they're rarely any good, but again, who cares? Its working, right?

Let's talk about what the job of a developer really is. Our responsibility goes beyond
just writing code, but we also don't just "solve problems". We're not glorified
"problem-solving-machines".

Let's summarise the job of a developer:
"A software developer maintains and evolves a technical solution to remain competative"

I know, it sounds boring, so let's unpack the statement and look closer at it:
By "maintaining" and "evolving" I simply mean the regular stuff: Update dependencies, fixing
bugs, adding new features, etc. Combined that concept with "competative" means that
these things need to happen in a reasonable timeframe as not to be outpaced by competitors.

But "competitive" does not just translate to companies. Software without users is more or less
pointless. So even free open-source projects needs to be competitive to have users.

Being "competitive" is not just about adding features. It also means doing so in a way
that doesn't generate a bunch of extra work. Let's say that for every change we make we also introduce
a bunch of new bugs, then we're both frustrating the users and also reducing our ability to evolve
the solution since every change results in us having to go back and re-do a bunch of work.

The word "remain" is also immensly important, because we need think about the long-term impact of our
code, not just the emidiate feature. We have to think about how we implement so that it
doesn't limit us in the future.

This means that our job is not only to create a solution that is working, but also one
that can be reliably changed over time. And to do this we need to consider a bunch of things:
- Release process
- Testing strategy
- Architecture
- Code readability
- Observability
- Etc

If all you do is having "working" as the bare minimum threshold, you end up in the tech debt
death spiral:

*TODO*

It is also perfectly okay not knowing what "good" even is, but try things out. Try different
approaches and see what reads better or what is more pleasant to work with. Try different
techniques and practices and see how it impacts your system. Don't be afraid to do the wrong
thing, you can always reverse the changes.

Speaking of reversing changes, remember to always use source control, okay? Thanks. 

I have three books that I can recommend for those that is interested in what it means to be
a professional developer that also touches on these topics:
- Clean Coder: A code of Conduct for Professional Programmers
- The Pragmatic Programmer: From Journeyman to Master
- The Software Craftsman: Professionalism, Pragmatism, Pride

Now, speaking about trying techniques and practices, let's talk about advice #3...

## Advice #3 - Don't _just_ learn the theory

In programming we have all sorts of pracices, techniques, principles, etc. All of which
tries to guide us towards writing better and more reliable software. When you enter
the industry you will find that a lot of developers doesn't actually know these,
or severly misunderstands them.

I'm not even talking about the obscure stuff, we can't even agree on OOP! I'd even go
so far as to say that 70-80% of developers can't provide an actual good definition of OOP in
a practical sense. They might know some of the theory, but they don't understand how to actually
apply the theory to code.

Now, it is fine for you to not know, but I've met with developers that has decades of experience,
yet completely clueless about one of the most fundemental and used paradigms in our industry.
I don't know if you've covered OOP yet, but is there anyone here that wants to give it a shot and
tell me what OOP is?

*ASK THE ROOM - Improvise?*

You see, to answer this question, you first need the theory, and history.

OOP was first coined by Alan Kay, but Kay has a very different view of OOP than we do today, so
OOP has two defintions:

 - Classical OOP
 - Modern OOP

 Classical OOP is what Kay was talking about. His idea of an "Object" is much more like we view
 microservices today. A bunch of processes runnin isolated from each other and communicating through
 messages. It is less about the programming language and more about the architecture. In this view, the
 language is more of a detail than anything else.
 
Modern OOP is what we got with languages like Java, C++, etc. It became less about messaging, and more
about the uses of classes. When we say OOP today we're more often than not referring to the modern
definition unless otherwise stated, and that is how I'll be using the term "OOP" for the rest of this session.

A while back I held a technical interviewed with someone that had a couple of decades YoE, and we got talking about paradigms and OOP. He was saying
something that didn't really make sense to me, so I asked him to give me his definition of OOP, whereupon
his answer was "Oh, isn't that like... classes and stuff?"

Not knowing answers to these basic things is, unfortunately, not a good look.

So, let's dig into it and let me demonstrate what it means to knowing something beyond the theory.

First off, what is the theory? Let's head over to good ol' reliable Wikipedia and say what it says:
"Object-oriented programming (OOP) is a programming paradigm based on objects â€“ software entities that encapsulate data and function(s)."

So, reading this sentence we know the theory. We have entities, often called classes, and we have data and functions. We also have encapsulation.

Cool, so what does that actually mean for our code? Let's take Java for exmaple, in Java, everything is an object. If you want some functionality you need to put them into objects. Does that mean that all code written in Java is inherintly OOP? Let's see.

Take a look at this examle:
```java
class Account {
    private double balance;

    public Account(double balance) {
        this.balance = balance;
    }

    public double getBalance() {
        return balance;
    }

    public void setBalance(double balance) {
        this.balance = balance;
    }
}

void transfer(Account from, Account to, double amount) {
    if (amount <= 0) return;

    if (from.getBalance() - amount < 0) {
        throw new IllegalStateException("Insufficient funds");
    }

    from.setBalance(from.getBalance() - amount);
    to.setBalance(to.getBalance() + amount);
}
```

Here we have a simplifed `Account` class for a banking system. An account has a balance,
and we're not allowed to go into the negative. We then need the functionality to transfer
money from one account to another, which exists somewhere else in the system, in some other class.

Is this OOP? *ASK THE ROOM - IMPROVISE*

No! Its not! This is procedural programming, not OOP. Sure, the word "classes" is involved,
but it does not adhere to OOP.

A dogmatic approach in OOP would be something like this:

```java
class Account {
    private double balance;

    public Account(double balance) {
        if (balance < 0) {
            throw new IllegalArgumentException("Negative balance");
        }
        this.balance = balance;
    }

    public double getBalance() {
        return balance;
    }

    public void withdraw(double amount) {
        if (amount <= 0) return;

        if (balance - amount < 0) {
            throw new IllegalStateException("Insufficient funds");
        }
        balance -= amount;
    }

    public void deposit(double amount) {
        if (amount <= 0) return;
        balance += amount;
    }

    public void transferTo(Account other, double amount) {
        withdraw(amount);
        other.deposit(amount);
    }
}
```

*Explain the code*

In this version we control how the balance of the account is changed, and we make sure
that it can never become an illegal value regardless of where the class is used in the system.

Now we can safely create new functionality that works with account balances without fearing that
they forget to validate, because the `Account` object itself ensure that it is always in the
correct state and that nobody gets to change it to something illegal.

The procedural implementation puts the burden of correctness on the caller, while the OOP
version puts the burden of correctness onto the object itself - and we achieve that by
combining data, functionality and encapsulation.

Given the above we can start to think about how we build our systems and use OOP to protect
our data from errors, and in turn protect ourselves from making future mistakes. And that
is what I mean by going beyond "just the theory". You need to understand what it is _AND_
how to apply it to the code you write. You need to understand what it gives you, and what
it takes away.

For example, in the OOP version the `Account` is much larger than the transaction script one,
and that is true. Nothing comes from free. There's no free lunch, and there's no silver bullet.
A very simple CRUD system might be better of being written in a procedural way, while a more
complex one might need the extra protections we can bake into our objects.

It is this deeper understanding that allows us to make decisions about which approach to take
and the tradeoffs it comes with. My description here isn't even complete, as there's so much
more we could dig into when it comes to OOP that builds on the basics above.

We just used OOP as an example of a paradigm which a lot of developers misunderstands, but
there are so many of them, for example:
- SRP does not mean "Every method/class does one thing".
- DRY does not mean "Never repeat yourself"
- The word unit in "unit tests" does not mean that unit tests only covers one class/method

The last advice here is to never assume what a practice/technique says. Read what it says,
and pay attention to what it doesn't say. I often see people adding bits that isn't present,
which is causing a lot of confusion.

## Advice #4 - Good design trumps perfect code with optimal performance

When I was a student here I was obsessed with performance. I wanted the fastest
implementation, and I was very hung up on following every practice to the letter.

In the real world, not so much.

The thing is, trying to write perfect code is an unachivable task. There's no such
thing. And in the real world the performance needs to be "good enough". A call taking
50ms instead of 55ms won't make much of a difference to the end user, so you quickly end
up with diminishin returns.

What is much more impactful is to think about how your codebase is internally wired up, and
how it communicates with the rest of the world. If you have a design that enables
change you have the room to both improve the code quality and its performance at any point
if needed.

A poorly structured system is prone to errors, and you're almost afraid of touching it.

TODO 

## Advice #5 - Write tests

When graduating from Aber I had written one test throughout my three years. A single test, which
was for a single assignment. The rest has no requirements for automated testing. I entered
the industry with little to no experience writing tests, and I'm not alone.

Testing has been very low on the priority list for most graduates and universities, and it is my
firm belif that automated testing should have a much higher priority. I still see graduates that have
never written a test whatsoever.

So let me try to sell you on testing, and how it can help you even in coursework that doesn't require it,
and then let's look at why later.

### Testing is feedback

So, let's take a look at the struggles I had when I was a student.

Let's say that I got this assigment. One with some heft. It needed a bunch of classes working together
and all that jazz.

So, how did I build it? Well, I first started by writing some code, but "some code" does not a full
application make. It wasn't runnable, because it was just a piece of the entire system, and hooking it
up to the output knowing that I would need to throw it away later seemed like more work. After all,
I could see what the next step was!

So I continued to write code until I had something that seemed correct and would get the job done.

Easy, right?

Well... the problem is that there's always a mistake somewhere. And now I have hours, if not days, of
work invested into this project, and more often than not I had to debug my way to the problem.

Sometimes I realized that I made the wrong assumption on day two of a one-week project!

Sometimes I added a dependency which I only realized broke the system once I finally got to run it.

There's a rule in this industry, and that is the more work you put into a project before verifying it,
the harder it'll be to figure out what is going wrong. So, the longer you go from starting the project
until you get to run and verify it, the more risk you take. You risk more work figuring out what
has gone wrong, and you risk having to undo more work.

This is where testing can help you, especially unit testing. With unit testing you can run a single method
separate from other methods. It allows you to write code while also verify and debugging it without
needing a "complete" system.

If I had written tests when I was a student I would've caught so many issues and overall saved
so much time. It is 100% worth it for larger assignments.

Let me demonstrate...

*TODO FIND OLD ASSIGNMENT AND SHOW HOW WE CAN DO IT?*